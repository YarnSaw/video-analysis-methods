{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the modules\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the datasets for use\n",
    "\n",
    "# given a video id, return the greyscale version of the video\n",
    "def processID(Id):\n",
    "  video = cv2.VideoCapture(f'subset/data/{Id}.webm')\n",
    "  \n",
    "  greyScaleVideo = []\n",
    "\n",
    "  # read each frame, convert the frame data to greyscale & append to the new array to be returned.\n",
    "  newFrame, data = video.read()\n",
    "  while newFrame:\n",
    "    greyScaleVideo.append(cv2.cvtColor(data, cv2.COLOR_BGR2GRAY))\n",
    "    newFrame, data = video.read()\n",
    "\n",
    "  return greyScaleVideo\n",
    "\n",
    "# training/validation data set json files (describes which videos belong to which category)\n",
    "with open('subset/subset-train.json') as file:\n",
    "  trainingSetInfo = json.load(file)\n",
    "\n",
    "with open('subset/subset-validation.json') as file:\n",
    "  validationSetInfo = json.load(file)\n",
    "\n",
    "\n",
    "#custom dataset class. Allows us to load only the data to be used into memory, since the videos are otherwise way too large\n",
    "class somethingDataset(data.Dataset):\n",
    "  #setInfo for either the training or validation set. The object loaded in above, used to know which videos belong to each set\n",
    "  def __init__(self, setInfo):\n",
    "    self.setInfo = setInfo\n",
    "  \n",
    "  # how many elements to the dataset\n",
    "  def __len__(self):\n",
    "    return len(self.setInfo)\n",
    "\n",
    "  # called whenever an item needs to be retrieved from the dataset. Current implementation is to\n",
    "  # find the video at the given index, load that video/convert to greyscale, then return that + the class (which is the template string)\n",
    "  def __getitem__(self, index):\n",
    "    vid = self.setInfo[index]\n",
    "    video = processID(vid['id'])\n",
    "    label = vid['template']\n",
    "    print(len(video))\n",
    "    return video, label\n",
    "\n",
    "# prepare the dataloaders that will work with the dataset class to get the specific data we want when we request it.\n",
    "# the first argument is the dataset we want to use (training or validation). Shuffle is if we want to randomize the order\n",
    "# of the dataset, and num_workers allows for some multithreading (should speed things up, but if we notice problems we can remove\n",
    "# so it will be slower but all on 1 thread)\n",
    "trainDataLoader = data.DataLoader(somethingDataset(trainingSetInfo), batch_size=64, shuffle=True, num_workers=3)\n",
    "validationDataLoader = data.DataLoader(validationSetInfo, batch_size=64, shuffle=True, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very simple convolutional neural network class.\n",
    "class CNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__() # instantiate the pytorch module\n",
    "\n",
    "    # Layer 1: a 3d convolution\n",
    "    #                      greyscale (3 for rgb)   num filters\n",
    "    self.conv1 = nn.Conv3d(in_channels=1,          out_channels=5, kernel_size=(5,5,5))\n",
    "    self.relu1 = nn.ReLU()\n",
    "    self.maxpool1 = nn.MaxPool3d(kernel_size=(2,2), stride=(2,2))\n",
    "\n",
    "    # Layer 2: linear (hidden)\n",
    "    # PROBABLE ERROR: NOT SURE WHAT SIZE THE in_features OF fc1 SHOULD BE. RIGHT NOW IT IS 800 AS JUST 'SOME BIG NUMBER', BUT PROBABLY NEED TO BE CHANGED. \n",
    "    # check the size of the output of flatten in the forward function to determine what this should be\n",
    "    self.fc1 = nn.Linear(in_features=800, out_features=500)\n",
    "    self.relu3 = nn.ReLU()\n",
    "\n",
    "    # Layer 3: linear (output)\n",
    "    #                                  have 3 classes\n",
    "    self.fc2 = nn.Linear(in_features=500, out_features=3)\n",
    "    self.logSoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "  # forward function for the model. Training using backward propagation, with the backward function being handled by\n",
    "  # pytorch so we only need to create the forward.\n",
    "  def forward(self, x):\n",
    "    # Layer 1: 3d convolution + maxpool\n",
    "    x = self.conv1(x)\n",
    "    x = self.relu1(x)\n",
    "    x = self.maxpool1(x)\n",
    "\n",
    "    # Layer 2: flatten the network, then use linear\n",
    "    x = torch.flatten(x,1)\n",
    "    x = self.fc1(x)\n",
    "    x = self.relu3(x)\n",
    "\n",
    "    # Layer 3: linear with softmax for our output\n",
    "    x = self.fc2(x)\n",
    "    output = self.logSoftmax(x)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the model\n",
    "\n",
    "\n",
    "device = 'cpu' # or 'cuda' in future\n",
    "model = CNN().to(device) # instantiate the CNN\n",
    "\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "loss = nn.CrossEntropyLoss() # loss function \n",
    "\n",
    "# optimizer function (what we are using to minimize the loss) (Adam is essentially a better version of gradient descent)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_loss_list = [] # we will want to keep a history of the loss values so we can plot it\n",
    "for epoch in range(EPOCHS):\n",
    "  train_loss = 0\n",
    "\n",
    "  model.train()\n",
    "  for (x,y) in trainDataLoader: # currently breaks here due to different size frames\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
